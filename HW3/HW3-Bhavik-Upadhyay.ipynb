{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e28af0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Callable\n",
    "from typing_extensions import TypeAlias\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "\n",
    "import os\n",
    "from urllib import request\n",
    "import gzip\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a5d313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f7e328",
   "metadata": {},
   "source": [
    "# Task 1: Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d3689e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
      "0          US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
      "1          US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
      "2          US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
      "3          US     52782374  R1PR37BR7G3M6A  B00D7H8XB6       868449945   \n",
      "4          US     24045652  R3BDDDZMZBZDPU  B001XCWP34        33521401   \n",
      "\n",
      "                                       product_title product_category  \\\n",
      "0     Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
      "1          Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
      "2  Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
      "3  AmazonBasics 12-Sheet High-Security Micro-Cut ...  Office Products   \n",
      "4  Derwent Colored Pencils, Inktense Ink Pencils,...  Office Products   \n",
      "\n",
      "  star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
      "0           5            0.0          0.0    N                 Y   \n",
      "1           5            0.0          1.0    N                 Y   \n",
      "2           5            0.0          0.0    N                 Y   \n",
      "3           1            2.0          3.0    N                 Y   \n",
      "4           4            0.0          0.0    N                 Y   \n",
      "\n",
      "                                     review_headline  \\\n",
      "0                                         Five Stars   \n",
      "1  Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
      "2                      but I am sure I will like it.   \n",
      "3  and the shredder was dirty and the bin was par...   \n",
      "4                                         Four Stars   \n",
      "\n",
      "                                         review_body review_date  \n",
      "0                                     Great product.  2015-08-31  \n",
      "1  What's to say about this commodity item except...  2015-08-31  \n",
      "2    Haven't used yet, but I am sure I will like it.  2015-08-31  \n",
      "3  Although this was labeled as &#34;new&#34; the...  2015-08-31  \n",
      "4                    Gorgeous colors and easy to use  2015-08-31  \n"
     ]
    }
   ],
   "source": [
    "url = 'https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz'\n",
    "\n",
    "extracted_file = 'data.tsv'\n",
    "compressed_file = extracted_file + '.gz'\n",
    "\n",
    "\n",
    "# Retrieve the dataset from given url and store it in location specified by compressed_file\n",
    "if not os.path.exists(extracted_file):\n",
    "    request.urlretrieve(url, compressed_file)\n",
    "\n",
    "    # extract the dataset from the gzipped file\n",
    "    with gzip.open(compressed_file, 'rb') as f_in, open(extracted_file, 'wb') as f_out:\n",
    "        for line in f_in:\n",
    "            f_out.write(line)\n",
    "\n",
    "    os.remove(compressed_file)\n",
    "    \n",
    "\n",
    "# read the extracted data into pandas dataframe\n",
    "original_df = pd.read_csv(extracted_file, sep='\\t', on_bad_lines='skip', low_memory=False)\n",
    "print(original_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261403c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         review_body star_rating\n",
      "0                                     Great product.           5\n",
      "1  What's to say about this commodity item except...           5\n",
      "2    Haven't used yet, but I am sure I will like it.           5\n",
      "3  Although this was labeled as &#34;new&#34; the...           1\n",
      "4                    Gorgeous colors and easy to use           4\n",
      "['5' '1' '4' '2' '3' '2015-06-05' '2015-02-11' nan '2014-02-14']\n",
      "[5. 1. 4. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "# creating the dataframe by taking only review_body and star_rating columns\n",
    "df = pd.DataFrame(original_df[['review_body', 'star_rating']])\n",
    "print(df.head())\n",
    "\n",
    "# we notice there are some erroneous values for the star_rating column\n",
    "print(df['star_rating'].unique())\n",
    "\n",
    "# converting the star_rating to numeric values and dropping erroneous columns\n",
    "df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(df['star_rating'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d86a74de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating the target column: target = 1 if star_rating is 1, 2 or 3. target = 2 if star_rating is 4 or 5\n",
    "df['star_rating'] = df['star_rating'].astype(int)\n",
    "df['target'] = df['star_rating'].apply(lambda x: 0 if x <= 3 else 1)\n",
    "\n",
    "sample_size = 50000\n",
    "\n",
    "# creating a sample dataframe where target = 1 of size 50000 rows\n",
    "class_1 = df.loc[df['target'] == 0].sample(n=sample_size, random_state=42)\n",
    "\n",
    "# creating a sample dataframe where target = 2 of size 50000 rows\n",
    "class_2 = df.loc[df['target'] == 1].sample(n=sample_size, random_state=42)\n",
    "\n",
    "# merging the two sample dataframes\n",
    "df_new = pd.concat([class_1, class_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f524f2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhavi\\AppData\\Local\\Temp\\ipykernel_17012\\1465821108.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(review, \"html.parser\")\n",
      "C:\\Users\\bhavi\\AppData\\Local\\Temp\\ipykernel_17012\\1465821108.py:13: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(review, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "def clean(review):\n",
    "    \"\"\"\n",
    "    convert to lower-case\n",
    "    remove html and urls\n",
    "    remove non-alphabetical character\n",
    "    remove extra spaces\n",
    "    \"\"\"\n",
    "    \n",
    "    # converting to lowercase\n",
    "    review = review.lower()\n",
    "    \n",
    "    # removing htmls\n",
    "    soup = BeautifulSoup(review, \"html.parser\")\n",
    "    \n",
    "    for a_tag in soup.find_all(\"a\"):\n",
    "        a_tag.decompose()\n",
    "        \n",
    "    review = soup.get_text()\n",
    "    \n",
    "    # removing urls\n",
    "    review = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', review)\n",
    "    \n",
    "    # removing non-alphabetical characters\n",
    "    review = re.sub(r'[^a-zA-Z\\s]', '', review)\n",
    "    \n",
    "    # removing extra spaces\n",
    "    review = re.sub(r'\\s+', ' ', review).strip()\n",
    "        \n",
    "    return review\n",
    "    \n",
    "\n",
    "df_new['review_body'] = df_new['review_body'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d563d03",
   "metadata": {},
   "source": [
    "# Task 2: Creating the Word2Vec Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a915daf5",
   "metadata": {},
   "source": [
    "When comparing a pretrained model with a custom word2vec model, we find a substantial difference in vocabulary size: 3,000,000 unique words in the pretrained model versus 14,994 in the custom model. This suggests the pretrained model may handle out-of-vocabulary words better during testing.\n",
    "\n",
    "In semantic similarity tests:\n",
    "1. Outstanding and excellent show lower similarity in the pretrained model.\n",
    "2. The arithmetic \"King - Man + Woman\" correctly yields \"Queen\" in the pretrained model, but not in the custom model.\n",
    "3. The arithmetic \"Doctor - Man + Woman\" doesn't produce nurse-related results in the custom model.\n",
    "\n",
    "Overall, the pretrained model performs better in some semantic tasks but struggles with similarity in specific cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585bb8b7",
   "metadata": {},
   "source": [
    "## Word2Vec from pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d87256a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# downloading the pre-trained model if it is not available\n",
    "if not os.path.exists('pretrained_w2v.model'):\n",
    "    pretrained_w2v = gensim.downloader.load('word2vec-google-news-300')\n",
    "    pretrained_w2v.save('pretrained_w2v.model')\n",
    "# if available, we load the data from local storage\n",
    "else:\n",
    "    pretrained_w2v = KeyedVectors.load('pretrained_w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d52abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000 300\n"
     ]
    }
   ],
   "source": [
    "print(len(pretrained_w2v), len(pretrained_w2v[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8b9d2b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55674857\n"
     ]
    }
   ],
   "source": [
    "# printing the similarity score for outstanding and excellent\n",
    "print(pretrained_w2v.similarity('outstanding', 'excellent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08ccb4c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321839332581)]\n"
     ]
    }
   ],
   "source": [
    "# printing the most similar words matching the arithmetic king - man + woman\n",
    "print(pretrained_w2v.most_similar(positive=['king', 'woman'], negative=['man'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b32f929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gynecologist', 0.7093892097473145), ('nurse', 0.6477287411689758), ('doctors', 0.6471460461616516), ('physician', 0.6438996195793152), ('pediatrician', 0.6249487996101379)]\n"
     ]
    }
   ],
   "source": [
    "# printing the most similar words matching the arithmetic doctor - man + queen\n",
    "print(pretrained_w2v.most_similar(positive=['doctor', 'woman'], negative=['man'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7153481a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000 300\n"
     ]
    }
   ],
   "source": [
    "# Creating the vocabulary from the pretrained w2v\n",
    "vocab = list(pretrained_w2v.index_to_key)\n",
    "\n",
    "# Initialize an embedding matrix with zeros\n",
    "embedding_dim = pretrained_w2v.vector_size\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim), dtype=np.float32)\n",
    "\n",
    "# Populate the embedding matrix with Word2Vec vectors\n",
    "for i, word in enumerate(vocab):\n",
    "    if word in pretrained_w2v:\n",
    "        # print(word, i, custom_w2v.wv[word])\n",
    "        embedding_matrix[i] = pretrained_w2v[word]\n",
    "\n",
    "print(len(embedding_matrix), len(embedding_matrix[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa68eec",
   "metadata": {},
   "source": [
    "## From dataset (custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d8a8eb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# getting the tokens\n",
    "tokenized = [nltk.word_tokenize(review) for review in df_new['review_body']]\n",
    "\n",
    "# if the model is not already available, then create from scratch\n",
    "if not os.path.exists('custom_w2v.model'):\n",
    "    custom_w2v = Word2Vec(tokenized, vector_size=300, window=13, min_count=9, sg=1, workers=1)\n",
    "    custom_w2v.save('custom_w2v.model')\n",
    "# if available, load the model from local storage\n",
    "else:\n",
    "    custom_w2v = Word2Vec.load('custom_w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab246024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14994 300\n"
     ]
    }
   ],
   "source": [
    "print(len(custom_w2v.wv), len(custom_w2v.wv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcf34039",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6205609\n"
     ]
    }
   ],
   "source": [
    "# printing the similarity score for outstanding and excellent\n",
    "print(custom_w2v.wv.similarity('outstanding', 'excellent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc6880ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Idea', 0.5137808918952942), ('Archival', 0.5026917457580566), ('fine-point', 0.49942412972450256), ('inherited', 0.4969730079174042), ('Flair', 0.4938381314277649)]\n"
     ]
    }
   ],
   "source": [
    "# printing the most similar words matching the arithmetic king - man + woman\n",
    "print(custom_w2v.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a73aade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('assistant', 0.4972769320011139), ('visiting', 0.4651092290878296), ('deadlines', 0.45854732394218445), ('appointment', 0.45726412534713745), ('prayer', 0.4571249485015869)]\n"
     ]
    }
   ],
   "source": [
    "# printing the most similar words matching the arithmetic doctor - man + queen\n",
    "print(custom_w2v.wv.most_similar(positive=['doctor', 'woman'], negative=['man'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee16eea",
   "metadata": {},
   "source": [
    "# Task 3: Simple Models\n",
    "\n",
    "\n",
    "* **Perceptron** achieved **80.77% accuracy** on word embeddings, which is slightly better than **79.345%** using TF-IDF.\n",
    "* **SVM** attained **82.665% accuracy** on word embeddings, slightly lower than **84.865%** using TF-IDF.\n",
    "\n",
    "* In general, word embeddings exhibit competitive performance, however, in the SVM task, we notice slightly less performance.\n",
    "\n",
    "**Note**: The two figures below show the accuracy obtained for perceptron and SVM when training on TF-IDF. These were obtained by modifying the metric of calculation in Homework 1 without any changes as to how the models were trained or features were extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b098b3",
   "metadata": {},
   "source": [
    "![Accuracy of perceptron on TF-IDF](img/perceptron_accuracy.png)\n",
    "<center><strong>Accuracy of perceptron on TF-IDF</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a95c47",
   "metadata": {},
   "source": [
    "![Accuracy for SVM on TF-IDF](img/svm_accuracy.png)\n",
    "<center><strong>Accuracy of SVM on TF-IDF</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6092d4e2",
   "metadata": {},
   "source": [
    "### Creating mean sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d876a0af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a function to create mean embeddings for a sentence given a w2v model\n",
    "def create_avg_embeddings(sentence, w2v):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    vectors = [w2v[word] for word in tokens if word in w2v]\n",
    "    \n",
    "    if vectors:\n",
    "        embedding = np.mean(vectors, axis=0, dtype=np.float32)\n",
    "    else:\n",
    "        embedding = np.zeros(w2v.vector_size, dtype=np.float32)\n",
    "        \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd2deb75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_embeddings = df_new['review_body'].apply(lambda x: create_avg_embeddings(x, pretrained_w2v))\n",
    "avg_embeddings = np.array(avg_embeddings.tolist())\n",
    "\n",
    "targets = df_new['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72cfbad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_avg, X_test_avg, Y_train_avg, Y_test_avg = train_test_split(\n",
    "    avg_embeddings,\n",
    "    df_new['target'],\n",
    "    shuffle=True,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "Y_train_avg = np.array(Y_train_avg.tolist())\n",
    "Y_test_avg = np.array(Y_test_avg.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ec3cae",
   "metadata": {},
   "source": [
    "### Perceptron training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46b8ad23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8077\n"
     ]
    }
   ],
   "source": [
    "per_clf = Perceptron(penalty='elasticnet', l1_ratio=0.8, alpha=1e-5, tol=1e-4, random_state=42)\n",
    "per_clf.fit(list(X_train_avg), Y_train_avg)\n",
    "\n",
    "per_Y_preds = per_clf.predict(list(X_test_avg))\n",
    "per_acc = accuracy_score(per_Y_preds, Y_test_avg)\n",
    "print(per_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0a58f",
   "metadata": {},
   "source": [
    "### SVM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c7af57d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82665\n"
     ]
    }
   ],
   "source": [
    "svc_clf = LinearSVC(dual=True, loss='hinge', C=0.8, max_iter=10000, random_state=42)\n",
    "svc_clf.fit(list(X_train_avg), Y_train_avg)\n",
    "\n",
    "svc_Y_preds = svc_clf.predict(list(X_test_avg))\n",
    "svc_acc = accuracy_score(svc_Y_preds, Y_test_avg)\n",
    "print(svc_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4616824d",
   "metadata": {},
   "source": [
    "# Task 4: Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72ba2580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# general purpose function for training a model with given training data \n",
    "def train(model, train_data, test_data, criterion, optimizer, n_epochs=10, verbose=True, device='cpu'):\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.\n",
    "        test_loss = 0.\n",
    "        \n",
    "        model.train()\n",
    "        for data, target in train_data:\n",
    "            # shifting data and target to the appropriate device\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            # setting the gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # getting the output and calculating the loss\n",
    "            out = model(data)\n",
    "            loss = criterion(out, target)\n",
    "            \n",
    "            # performing the backward step and using the optimizer\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_data:\n",
    "                # shifting data and target to appropriate device\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                # getting the output and then getting the loss and updating the total test loss\n",
    "                out = model(data)\n",
    "                \n",
    "                loss = criterion(out, target)\n",
    "                test_loss += loss.item()\n",
    "                \n",
    "        if verbose:\n",
    "            print(f'Epoch: {epoch+1} / {n_epochs}\\tTraining Loss: {train_loss}\\tTest Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e8e8d1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a function to calculate accuracy on test data for a given model\n",
    "def accuracy(model, test_data, device='cpu'):\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_data:\n",
    "            # shifting data and target to appropriate device\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            # getting the output and predictions\n",
    "            out = model(data)\n",
    "            _, preds = torch.max(out.data, 1)\n",
    "            \n",
    "            # updating the total and correct variables\n",
    "            total += target.size(0)\n",
    "            correct += (preds == target).sum().item()\n",
    "            \n",
    "    return (100*correct) / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69d8be03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# converting numpy arrays created for perceptron and svm to torch tensors\n",
    "X_train_avg = torch.tensor(X_train_avg, dtype=torch.float32)\n",
    "X_test_avg = torch.tensor(X_test_avg, dtype=torch.float32)\n",
    "\n",
    "Y_train_avg = torch.tensor(Y_train_avg, dtype=torch.int64)\n",
    "Y_test_avg = torch.tensor(Y_test_avg, dtype=torch.int64)\n",
    "\n",
    "# generating dataset from created tensors\n",
    "train_dataset1 = TensorDataset(X_train_avg, Y_train_avg)\n",
    "test_dataset1 = TensorDataset(X_test_avg, Y_test_avg)\n",
    "\n",
    "batch_size = 256\n",
    "# creating train and test data loaders\n",
    "train_loader1 = DataLoader(train_dataset1, batch_size=batch_size, shuffle=True)\n",
    "test_loader1 = DataLoader(test_dataset1, batch_size=batch_size)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c01347",
   "metadata": {},
   "source": [
    "## Task 4 (a)\n",
    "* Here, we train a feedforward neural network on sentence embeddings obtained by calculating the mean of all word embeddings in the sentence.\n",
    "* We train the neural network using AdamW optimizer with 1e-4 learning rate for 100 epochs with batch size 256.\n",
    "* The accuracy on test set for this model is roughly between **82% - 84%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a1ff80e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork1(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=5, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork1, self).__init__()\n",
    "        self.embedding_dim = 300\n",
    "        self.hidden1 = 50\n",
    "        self.hidden2 = 5\n",
    "        self.out_dim = 2\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden1, self.hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden2, self.out_dim),\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model1 = NeuralNetwork1()\n",
    "model1.to(device)\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c3d92d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model1.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddb3d2f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 100\tTraining Loss: 223.27994138002396\tTest Loss: 54.716657280921936\n",
      "Epoch: 2 / 100\tTraining Loss: 205.70290875434875\tTest Loss: 48.29630637168884\n",
      "Epoch: 3 / 100\tTraining Loss: 176.52640929818153\tTest Loss: 41.207524448633194\n",
      "Epoch: 4 / 100\tTraining Loss: 155.0030519068241\tTest Loss: 37.60107463598251\n",
      "Epoch: 5 / 100\tTraining Loss: 144.89616572856903\tTest Loss: 35.95973256230354\n",
      "Epoch: 6 / 100\tTraining Loss: 140.06317156553268\tTest Loss: 35.0801557302475\n",
      "Epoch: 7 / 100\tTraining Loss: 137.1439170241356\tTest Loss: 34.57513916492462\n",
      "Epoch: 8 / 100\tTraining Loss: 135.18273961544037\tTest Loss: 34.08438017964363\n",
      "Epoch: 9 / 100\tTraining Loss: 133.62331506609917\tTest Loss: 33.74214479327202\n",
      "Epoch: 10 / 100\tTraining Loss: 132.32582929730415\tTest Loss: 33.46182382106781\n",
      "Epoch: 11 / 100\tTraining Loss: 131.22946453094482\tTest Loss: 33.21343466639519\n",
      "Epoch: 12 / 100\tTraining Loss: 130.28965264558792\tTest Loss: 32.998418152332306\n",
      "Epoch: 13 / 100\tTraining Loss: 129.46428593993187\tTest Loss: 32.80886098742485\n",
      "Epoch: 14 / 100\tTraining Loss: 128.77806401252747\tTest Loss: 32.64651048183441\n",
      "Epoch: 15 / 100\tTraining Loss: 128.14307433366776\tTest Loss: 32.497187316417694\n",
      "Epoch: 16 / 100\tTraining Loss: 127.5267682671547\tTest Loss: 32.354840099811554\n",
      "Epoch: 17 / 100\tTraining Loss: 126.88095933198929\tTest Loss: 32.239379823207855\n",
      "Epoch: 18 / 100\tTraining Loss: 126.27783998847008\tTest Loss: 32.22042775154114\n",
      "Epoch: 19 / 100\tTraining Loss: 125.91750055551529\tTest Loss: 31.992936491966248\n",
      "Epoch: 20 / 100\tTraining Loss: 125.32829281687737\tTest Loss: 31.872982531785965\n",
      "Epoch: 21 / 100\tTraining Loss: 124.9611741900444\tTest Loss: 31.765440344810486\n",
      "Epoch: 22 / 100\tTraining Loss: 124.49292731285095\tTest Loss: 31.662551641464233\n",
      "Epoch: 23 / 100\tTraining Loss: 124.03318306803703\tTest Loss: 31.565906643867493\n",
      "Epoch: 24 / 100\tTraining Loss: 123.64501696825027\tTest Loss: 31.490920424461365\n",
      "Epoch: 25 / 100\tTraining Loss: 123.26009619235992\tTest Loss: 31.418159753084183\n",
      "Epoch: 26 / 100\tTraining Loss: 122.91045781970024\tTest Loss: 31.33314400911331\n",
      "Epoch: 27 / 100\tTraining Loss: 122.6216288805008\tTest Loss: 31.26932805776596\n",
      "Epoch: 28 / 100\tTraining Loss: 122.26745873689651\tTest Loss: 31.225706696510315\n",
      "Epoch: 29 / 100\tTraining Loss: 121.92679944634438\tTest Loss: 31.129334658384323\n",
      "Epoch: 30 / 100\tTraining Loss: 121.6411349773407\tTest Loss: 31.135849595069885\n",
      "Epoch: 31 / 100\tTraining Loss: 121.3110009431839\tTest Loss: 31.019828230142593\n",
      "Epoch: 32 / 100\tTraining Loss: 121.060700237751\tTest Loss: 30.958229959011078\n",
      "Epoch: 33 / 100\tTraining Loss: 120.7994986474514\tTest Loss: 30.945522993803024\n",
      "Epoch: 34 / 100\tTraining Loss: 120.50094133615494\tTest Loss: 30.85480245947838\n",
      "Epoch: 35 / 100\tTraining Loss: 120.2346733212471\tTest Loss: 30.826304495334625\n",
      "Epoch: 36 / 100\tTraining Loss: 120.0566368997097\tTest Loss: 30.808255553245544\n",
      "Epoch: 37 / 100\tTraining Loss: 119.85122066736221\tTest Loss: 30.851676404476166\n",
      "Epoch: 38 / 100\tTraining Loss: 119.64552560448647\tTest Loss: 30.70193523168564\n",
      "Epoch: 39 / 100\tTraining Loss: 119.43237388134003\tTest Loss: 30.66961708664894\n",
      "Epoch: 40 / 100\tTraining Loss: 119.18115195631981\tTest Loss: 30.64968267083168\n",
      "Epoch: 41 / 100\tTraining Loss: 119.03138810396194\tTest Loss: 30.572869837284088\n",
      "Epoch: 42 / 100\tTraining Loss: 118.7706771492958\tTest Loss: 30.539514303207397\n",
      "Epoch: 43 / 100\tTraining Loss: 118.66438245773315\tTest Loss: 30.56415206193924\n",
      "Epoch: 44 / 100\tTraining Loss: 118.44496589899063\tTest Loss: 30.49730333685875\n",
      "Epoch: 45 / 100\tTraining Loss: 118.27230963110924\tTest Loss: 30.445301681756973\n",
      "Epoch: 46 / 100\tTraining Loss: 118.13230195641518\tTest Loss: 30.425574600696564\n",
      "Epoch: 47 / 100\tTraining Loss: 117.95960614085197\tTest Loss: 30.442767202854156\n",
      "Epoch: 48 / 100\tTraining Loss: 117.84438782930374\tTest Loss: 30.37128135561943\n",
      "Epoch: 49 / 100\tTraining Loss: 117.6802129149437\tTest Loss: 30.35201469063759\n",
      "Epoch: 50 / 100\tTraining Loss: 117.48000007867813\tTest Loss: 30.369530647993088\n",
      "Epoch: 51 / 100\tTraining Loss: 117.35213413834572\tTest Loss: 30.313990354537964\n",
      "Epoch: 52 / 100\tTraining Loss: 117.1872521340847\tTest Loss: 30.272311061620712\n",
      "Epoch: 53 / 100\tTraining Loss: 117.05581396818161\tTest Loss: 30.287183433771133\n",
      "Epoch: 54 / 100\tTraining Loss: 116.89013743400574\tTest Loss: 30.219446301460266\n",
      "Epoch: 55 / 100\tTraining Loss: 116.75635251402855\tTest Loss: 30.19229105114937\n",
      "Epoch: 56 / 100\tTraining Loss: 116.64956653118134\tTest Loss: 30.226687908172607\n",
      "Epoch: 57 / 100\tTraining Loss: 116.44064235687256\tTest Loss: 30.174104303121567\n",
      "Epoch: 58 / 100\tTraining Loss: 116.2725133895874\tTest Loss: 30.144307047128677\n",
      "Epoch: 59 / 100\tTraining Loss: 116.18694359064102\tTest Loss: 30.14928701519966\n",
      "Epoch: 60 / 100\tTraining Loss: 116.06538730859756\tTest Loss: 30.069722801446915\n",
      "Epoch: 61 / 100\tTraining Loss: 115.90143203735352\tTest Loss: 30.04452556371689\n",
      "Epoch: 62 / 100\tTraining Loss: 115.8383446931839\tTest Loss: 30.03296783566475\n",
      "Epoch: 63 / 100\tTraining Loss: 115.65640524029732\tTest Loss: 30.038346081972122\n",
      "Epoch: 64 / 100\tTraining Loss: 115.46543255448341\tTest Loss: 29.980317026376724\n",
      "Epoch: 65 / 100\tTraining Loss: 115.36341765522957\tTest Loss: 29.972163796424866\n",
      "Epoch: 66 / 100\tTraining Loss: 115.20414933562279\tTest Loss: 29.93922859430313\n",
      "Epoch: 67 / 100\tTraining Loss: 115.07365503907204\tTest Loss: 29.927978098392487\n",
      "Epoch: 68 / 100\tTraining Loss: 114.92494955658913\tTest Loss: 29.88445019721985\n",
      "Epoch: 69 / 100\tTraining Loss: 114.82510590553284\tTest Loss: 29.96846315264702\n",
      "Epoch: 70 / 100\tTraining Loss: 114.70393919944763\tTest Loss: 29.846114546060562\n",
      "Epoch: 71 / 100\tTraining Loss: 114.50519832968712\tTest Loss: 29.823754519224167\n",
      "Epoch: 72 / 100\tTraining Loss: 114.43318873643875\tTest Loss: 29.961664497852325\n",
      "Epoch: 73 / 100\tTraining Loss: 114.29965949058533\tTest Loss: 29.841556757688522\n",
      "Epoch: 74 / 100\tTraining Loss: 114.14315912127495\tTest Loss: 29.775625854730606\n",
      "Epoch: 75 / 100\tTraining Loss: 114.02090355753899\tTest Loss: 29.751518547534943\n",
      "Epoch: 76 / 100\tTraining Loss: 113.9025110900402\tTest Loss: 29.726153671741486\n",
      "Epoch: 77 / 100\tTraining Loss: 113.76714563369751\tTest Loss: 29.707972019910812\n",
      "Epoch: 78 / 100\tTraining Loss: 113.66112577915192\tTest Loss: 29.693700343370438\n",
      "Epoch: 79 / 100\tTraining Loss: 113.52604535222054\tTest Loss: 29.68906545639038\n",
      "Epoch: 80 / 100\tTraining Loss: 113.44295200705528\tTest Loss: 29.694132387638092\n",
      "Epoch: 81 / 100\tTraining Loss: 113.22596323490143\tTest Loss: 29.699741423130035\n",
      "Epoch: 82 / 100\tTraining Loss: 113.16738465428352\tTest Loss: 29.632641434669495\n",
      "Epoch: 83 / 100\tTraining Loss: 113.02845710515976\tTest Loss: 29.603986263275146\n",
      "Epoch: 84 / 100\tTraining Loss: 112.89375838637352\tTest Loss: 29.577137231826782\n",
      "Epoch: 85 / 100\tTraining Loss: 112.86698698997498\tTest Loss: 29.59031268954277\n",
      "Epoch: 86 / 100\tTraining Loss: 112.68282690644264\tTest Loss: 29.568561047315598\n",
      "Epoch: 87 / 100\tTraining Loss: 112.57322052121162\tTest Loss: 29.532892733812332\n",
      "Epoch: 88 / 100\tTraining Loss: 112.46350705623627\tTest Loss: 29.514625161886215\n",
      "Epoch: 89 / 100\tTraining Loss: 112.35060584545135\tTest Loss: 29.497875690460205\n",
      "Epoch: 90 / 100\tTraining Loss: 112.22641348838806\tTest Loss: 29.4856516122818\n",
      "Epoch: 91 / 100\tTraining Loss: 112.12844929099083\tTest Loss: 29.459724247455597\n",
      "Epoch: 92 / 100\tTraining Loss: 111.98545953631401\tTest Loss: 29.45690569281578\n",
      "Epoch: 93 / 100\tTraining Loss: 111.86591139435768\tTest Loss: 29.447207391262054\n",
      "Epoch: 94 / 100\tTraining Loss: 111.7561075091362\tTest Loss: 29.399598449468613\n",
      "Epoch: 95 / 100\tTraining Loss: 111.67023974657059\tTest Loss: 29.484870731830597\n",
      "Epoch: 96 / 100\tTraining Loss: 111.57436427474022\tTest Loss: 29.38424116373062\n",
      "Epoch: 97 / 100\tTraining Loss: 111.46817779541016\tTest Loss: 29.367492109537125\n",
      "Epoch: 98 / 100\tTraining Loss: 111.29240453243256\tTest Loss: 29.386514335870743\n",
      "Epoch: 99 / 100\tTraining Loss: 111.14512953162193\tTest Loss: 29.411173075437546\n",
      "Epoch: 100 / 100\tTraining Loss: 111.15172135829926\tTest Loss: 29.344766169786453\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "train(model1, train_loader1, test_loader1, criterion, optimizer, n_epochs=100, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f899f307",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.965\n"
     ]
    }
   ],
   "source": [
    "part_4a_accuracy = accuracy(model1, test_loader1, device)\n",
    "print(part_4a_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38151772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "def create_pad_embeddings(sentence, w2v, max_len=10):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    vec_size = w2v.vector_size\n",
    "    \n",
    "    embedding = np.zeros((max_len * vec_size), dtype=np.float32)\n",
    "    for i, word in enumerate(tokens):\n",
    "        if i >= max_len:\n",
    "            break\n",
    "            \n",
    "        if word in w2v:\n",
    "            embedding[i*vec_size: (i+1)*vec_size] = w2v[word]\n",
    "                  \n",
    "    return embedding\n",
    "\n",
    "sentence = \"This is a sample sentence\"\n",
    "sample_embedding = create_pad_embeddings(sentence, pretrained_w2v)\n",
    "print(sample_embedding.shape)\n",
    "\n",
    "print(create_avg_embeddings(sentence, pretrained_w2v).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a48bbe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pad_embeddings = df_new['review_body'].apply(lambda x: create_pad_embeddings(x, pretrained_w2v))\n",
    "pad_embeddings = np.array(pad_embeddings.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4698a683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_pad, X_test_pad, Y_train_pad, Y_test_pad = train_test_split(\n",
    "    pad_embeddings,\n",
    "    targets,\n",
    "    shuffle=True, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "Y_train_pad = np.array(Y_train_pad.tolist())\n",
    "Y_test_pad = np.array(Y_test_pad.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4fcc9ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_pad = torch.tensor(X_train_pad, dtype=torch.float32)\n",
    "X_test_pad = torch.tensor(X_test_pad, dtype=torch.float32)\n",
    "\n",
    "Y_train_pad = torch.tensor(Y_train_pad, dtype=torch.int64)\n",
    "Y_test_pad = torch.tensor(Y_test_pad, dtype=torch.int64)\n",
    "\n",
    "train_dataset2 = TensorDataset(X_train_pad, Y_train_pad)\n",
    "test_dataset2 = TensorDataset(X_test_pad, Y_test_pad)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader2 = DataLoader(train_dataset2, batch_size=batch_size, shuffle=True)\n",
    "test_loader2 = DataLoader(test_dataset2, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d8fa81",
   "metadata": {},
   "source": [
    "## Task 4 (b)\n",
    "\n",
    "* Here, we train a feedforward neural network on sentence embeddings obtained by concatenating the first 10 word embeddings and applying padding if the sentence is smaller than 10 words.\n",
    "* We use a ReLU activation layer between the linear layers.\n",
    "* We train the neural network using AdamW optimizer with 1e-4 learning rate and 1e-4 weight decay for 30 epochs with batch size 256.\n",
    "* Upon re-running the code several times, we observe the accuracy on test set for this model is roughly between **75.5% - 76.6%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6af7b193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork2(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=3000, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=5, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork2, self).__init__()\n",
    "        self.embedding_dim = 3000\n",
    "        self.hidden1 = 50\n",
    "        self.hidden2 = 5\n",
    "        self.out_dim = 2\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden1, self.hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden2, self.out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model2 = NeuralNetwork2()\n",
    "model2.to(device)\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29a1feb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model2.parameters(), lr=1e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a80b9ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 30\tTraining Loss: 194.17466282844543\tTest Loss: 43.72030174732208\n",
      "Epoch: 2 / 30\tTraining Loss: 165.8945385813713\tTest Loss: 41.193147748708725\n",
      "Epoch: 3 / 30\tTraining Loss: 159.00936275720596\tTest Loss: 40.39856415987015\n",
      "Epoch: 4 / 30\tTraining Loss: 155.71505045890808\tTest Loss: 39.918146044015884\n",
      "Epoch: 5 / 30\tTraining Loss: 153.38784858584404\tTest Loss: 39.65698781609535\n",
      "Epoch: 6 / 30\tTraining Loss: 151.5978156030178\tTest Loss: 39.41517451405525\n",
      "Epoch: 7 / 30\tTraining Loss: 149.9291484951973\tTest Loss: 39.31141784787178\n",
      "Epoch: 8 / 30\tTraining Loss: 148.38147443532944\tTest Loss: 39.11142221093178\n",
      "Epoch: 9 / 30\tTraining Loss: 146.90436762571335\tTest Loss: 39.007114231586456\n",
      "Epoch: 10 / 30\tTraining Loss: 145.52796256542206\tTest Loss: 38.82473134994507\n",
      "Epoch: 11 / 30\tTraining Loss: 144.09228175878525\tTest Loss: 38.75255364179611\n",
      "Epoch: 12 / 30\tTraining Loss: 142.6417380273342\tTest Loss: 38.639875173568726\n",
      "Epoch: 13 / 30\tTraining Loss: 141.23964083194733\tTest Loss: 38.59825983643532\n",
      "Epoch: 14 / 30\tTraining Loss: 140.0570511519909\tTest Loss: 38.49330136179924\n",
      "Epoch: 15 / 30\tTraining Loss: 138.561135917902\tTest Loss: 38.5560596883297\n",
      "Epoch: 16 / 30\tTraining Loss: 137.25511133670807\tTest Loss: 38.54057967662811\n",
      "Epoch: 17 / 30\tTraining Loss: 135.8898992240429\tTest Loss: 38.45652109384537\n",
      "Epoch: 18 / 30\tTraining Loss: 134.39312362670898\tTest Loss: 38.396814465522766\n",
      "Epoch: 19 / 30\tTraining Loss: 133.03255796432495\tTest Loss: 38.37224414944649\n",
      "Epoch: 20 / 30\tTraining Loss: 131.66759631037712\tTest Loss: 38.39131438732147\n",
      "Epoch: 21 / 30\tTraining Loss: 130.14192607998848\tTest Loss: 38.43498423695564\n",
      "Epoch: 22 / 30\tTraining Loss: 128.69674035906792\tTest Loss: 38.42015391588211\n",
      "Epoch: 23 / 30\tTraining Loss: 127.28686335682869\tTest Loss: 38.41508102416992\n",
      "Epoch: 24 / 30\tTraining Loss: 125.83272141218185\tTest Loss: 38.49851316213608\n",
      "Epoch: 25 / 30\tTraining Loss: 124.14575991034508\tTest Loss: 38.61513492465019\n",
      "Epoch: 26 / 30\tTraining Loss: 122.76370960474014\tTest Loss: 38.81642150878906\n",
      "Epoch: 27 / 30\tTraining Loss: 121.15855580568314\tTest Loss: 38.708027839660645\n",
      "Epoch: 28 / 30\tTraining Loss: 119.53005722165108\tTest Loss: 38.78369262814522\n",
      "Epoch: 29 / 30\tTraining Loss: 117.86586755514145\tTest Loss: 38.875079065561295\n",
      "Epoch: 30 / 30\tTraining Loss: 116.18799751996994\tTest Loss: 38.919744431972504\n"
     ]
    }
   ],
   "source": [
    "train(model2, train_loader2, test_loader2, criterion=criterion, optimizer=optimizer, n_epochs=30, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc17f527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.29\n"
     ]
    }
   ],
   "source": [
    "part_4b_accuracy = accuracy(model2, test_loader2, device=device)\n",
    "print(part_4b_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6342540a",
   "metadata": {},
   "source": [
    "# Task 5: Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "059b6508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_block_embeddings(sentence, w2v, max_len=10):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    vec_size = w2v.vector_size\n",
    "    \n",
    "    embedding = np.zeros((max_len, vec_size), dtype=np.float32)\n",
    "    for i, word in enumerate(tokens):\n",
    "        if i >= max_len:\n",
    "            break\n",
    "            \n",
    "        if word in w2v:\n",
    "            embedding[i] = w2v[word]\n",
    "                    \n",
    "    return embedding\n",
    "\n",
    "sentence = \"This is a sample sentence\"\n",
    "sample_embedding = create_block_embeddings(sentence, pretrained_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55cdc6b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "block_embeddings = df_new['review_body'].apply(lambda x: create_block_embeddings(x, pretrained_w2v))\n",
    "block_embeddings = np.array(block_embeddings.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8f94ce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_block, X_test_block, Y_train_block, Y_test_block = train_test_split(\n",
    "    block_embeddings,\n",
    "    targets,\n",
    "    shuffle=True,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "Y_train_block = np.array(Y_train_block.tolist())\n",
    "Y_test_block = np.array(Y_test_block.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b9bd52c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_block = torch.tensor(X_train_block, dtype=torch.float32)\n",
    "X_test_block = torch.tensor(X_test_block, dtype=torch.float32)\n",
    "\n",
    "Y_train_block = torch.tensor(Y_train_block, dtype=torch.int64)\n",
    "Y_test_block = torch.tensor(Y_test_block, dtype=torch.int64)\n",
    "\n",
    "train_dataset3 = TensorDataset(X_train_block, Y_train_block)\n",
    "test_dataset3 = TensorDataset(X_test_block, Y_test_block)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader3 = DataLoader(train_dataset3, batch_size=batch_size, shuffle=True)\n",
    "test_loader3 = DataLoader(test_dataset3, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae76e0a",
   "metadata": {},
   "source": [
    "## Task 5 (a)\n",
    "* For this task, we train a RNN on sentence embeddings obtained by stacking the first 10 word embeddings and applying padding if the sentence is smaller than 10 words.\n",
    "* We train the neural network using AdamW optimizer with 1e-3 learning rate with 1e-6 weight decay for 100 epochs with batch size 256.\n",
    "* Upon re-running the code several times, we observe the accuracy on test set for this model is roughly around **77.7% - 78.5%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "489fd8e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNClf(\n",
      "  (rnn): RNN(300, 10, batch_first=True)\n",
      "  (linear): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNNClf(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNClf, self).__init__()\n",
    "        self.embed_size = 300\n",
    "        self.hidden_size = 10\n",
    "        self.out_size = 2\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=self.embed_size, hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, hidden = self.rnn(x)\n",
    "        x = self.linear(x[:, -1, :])\n",
    "\n",
    "        return x\n",
    "    \n",
    "rnnModel = RNNClf()\n",
    "rnnModel.to(device)\n",
    "print(rnnModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c3cc41b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(rnnModel.parameters(), lr=1e-3, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "965c7d62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 100\tTraining Loss: 189.24623772501945\tTest Loss: 42.86137253046036\n",
      "Epoch: 2 / 100\tTraining Loss: 165.2519319653511\tTest Loss: 41.2786665558815\n",
      "Epoch: 3 / 100\tTraining Loss: 160.46886545419693\tTest Loss: 40.46919998526573\n",
      "Epoch: 4 / 100\tTraining Loss: 158.11708521842957\tTest Loss: 40.11415392160416\n",
      "Epoch: 5 / 100\tTraining Loss: 156.1327583193779\tTest Loss: 39.94943976402283\n",
      "Epoch: 6 / 100\tTraining Loss: 154.3838656246662\tTest Loss: 39.207602590322495\n",
      "Epoch: 7 / 100\tTraining Loss: 153.1704162955284\tTest Loss: 39.585414320230484\n",
      "Epoch: 8 / 100\tTraining Loss: 151.9054266512394\tTest Loss: 38.63105762004852\n",
      "Epoch: 9 / 100\tTraining Loss: 150.69831427931786\tTest Loss: 38.46751955151558\n",
      "Epoch: 10 / 100\tTraining Loss: 149.9881165921688\tTest Loss: 38.16771939396858\n",
      "Epoch: 11 / 100\tTraining Loss: 149.20873829722404\tTest Loss: 38.12427684664726\n",
      "Epoch: 12 / 100\tTraining Loss: 148.06970876455307\tTest Loss: 37.88624459505081\n",
      "Epoch: 13 / 100\tTraining Loss: 147.25514441728592\tTest Loss: 37.83639848232269\n",
      "Epoch: 14 / 100\tTraining Loss: 147.02438268065453\tTest Loss: 37.81947907805443\n",
      "Epoch: 15 / 100\tTraining Loss: 146.1519265472889\tTest Loss: 37.40324741601944\n",
      "Epoch: 16 / 100\tTraining Loss: 145.35491436719894\tTest Loss: 37.36286148428917\n",
      "Epoch: 17 / 100\tTraining Loss: 144.6717321574688\tTest Loss: 37.37463703751564\n",
      "Epoch: 18 / 100\tTraining Loss: 143.9930343925953\tTest Loss: 37.337947338819504\n",
      "Epoch: 19 / 100\tTraining Loss: 143.5869961977005\tTest Loss: 37.078449696302414\n",
      "Epoch: 20 / 100\tTraining Loss: 143.13337874412537\tTest Loss: 37.605335503816605\n",
      "Epoch: 21 / 100\tTraining Loss: 142.94152796268463\tTest Loss: 37.53921481966972\n",
      "Epoch: 22 / 100\tTraining Loss: 142.4502227306366\tTest Loss: 36.98101082444191\n",
      "Epoch: 23 / 100\tTraining Loss: 141.7295179963112\tTest Loss: 36.969017028808594\n",
      "Epoch: 24 / 100\tTraining Loss: 141.79342359304428\tTest Loss: 36.92888504266739\n",
      "Epoch: 25 / 100\tTraining Loss: 141.24457421898842\tTest Loss: 36.921512484550476\n",
      "Epoch: 26 / 100\tTraining Loss: 140.69035521149635\tTest Loss: 36.67476940155029\n",
      "Epoch: 27 / 100\tTraining Loss: 140.56993076205254\tTest Loss: 36.67093303799629\n",
      "Epoch: 28 / 100\tTraining Loss: 140.12270125746727\tTest Loss: 36.673606127500534\n",
      "Epoch: 29 / 100\tTraining Loss: 140.30863592028618\tTest Loss: 36.74337786436081\n",
      "Epoch: 30 / 100\tTraining Loss: 139.8257461488247\tTest Loss: 36.717282712459564\n",
      "Epoch: 31 / 100\tTraining Loss: 139.38076090812683\tTest Loss: 36.759630620479584\n",
      "Epoch: 32 / 100\tTraining Loss: 139.37510937452316\tTest Loss: 36.75336068868637\n",
      "Epoch: 33 / 100\tTraining Loss: 138.59608009457588\tTest Loss: 36.559115529060364\n",
      "Epoch: 34 / 100\tTraining Loss: 138.47659105062485\tTest Loss: 36.69157314300537\n",
      "Epoch: 35 / 100\tTraining Loss: 138.35360470414162\tTest Loss: 36.672885090112686\n",
      "Epoch: 36 / 100\tTraining Loss: 138.4716020822525\tTest Loss: 36.34186860918999\n",
      "Epoch: 37 / 100\tTraining Loss: 138.0806143283844\tTest Loss: 36.49436393380165\n",
      "Epoch: 38 / 100\tTraining Loss: 137.92330566048622\tTest Loss: 36.363194674253464\n",
      "Epoch: 39 / 100\tTraining Loss: 137.7402704358101\tTest Loss: 36.36631777882576\n",
      "Epoch: 40 / 100\tTraining Loss: 137.59592580795288\tTest Loss: 36.46004328131676\n",
      "Epoch: 41 / 100\tTraining Loss: 137.2826405465603\tTest Loss: 36.5595398247242\n",
      "Epoch: 42 / 100\tTraining Loss: 137.12348607182503\tTest Loss: 36.4635514318943\n",
      "Epoch: 43 / 100\tTraining Loss: 137.01033291220665\tTest Loss: 36.34114542603493\n",
      "Epoch: 44 / 100\tTraining Loss: 136.74183830618858\tTest Loss: 36.264641135931015\n",
      "Epoch: 45 / 100\tTraining Loss: 136.3272634446621\tTest Loss: 36.134812384843826\n",
      "Epoch: 46 / 100\tTraining Loss: 136.4916720688343\tTest Loss: 36.68380865454674\n",
      "Epoch: 47 / 100\tTraining Loss: 136.05500841140747\tTest Loss: 36.26983544230461\n",
      "Epoch: 48 / 100\tTraining Loss: 135.8873808979988\tTest Loss: 36.39075103402138\n",
      "Epoch: 49 / 100\tTraining Loss: 136.05873787403107\tTest Loss: 36.15823784470558\n",
      "Epoch: 50 / 100\tTraining Loss: 135.63928240537643\tTest Loss: 36.31730031967163\n",
      "Epoch: 51 / 100\tTraining Loss: 136.06965699791908\tTest Loss: 36.068804532289505\n",
      "Epoch: 52 / 100\tTraining Loss: 135.45133262872696\tTest Loss: 36.76198589801788\n",
      "Epoch: 53 / 100\tTraining Loss: 135.2078354358673\tTest Loss: 36.18658712506294\n",
      "Epoch: 54 / 100\tTraining Loss: 135.38859137892723\tTest Loss: 36.101177006959915\n",
      "Epoch: 55 / 100\tTraining Loss: 135.46006244421005\tTest Loss: 36.34166017174721\n",
      "Epoch: 56 / 100\tTraining Loss: 135.55552461743355\tTest Loss: 35.95088368654251\n",
      "Epoch: 57 / 100\tTraining Loss: 135.10890033841133\tTest Loss: 37.04864928126335\n",
      "Epoch: 58 / 100\tTraining Loss: 134.89186203479767\tTest Loss: 36.29133144021034\n",
      "Epoch: 59 / 100\tTraining Loss: 135.04605770111084\tTest Loss: 36.06994870305061\n",
      "Epoch: 60 / 100\tTraining Loss: 134.73051998019218\tTest Loss: 36.03017449378967\n",
      "Epoch: 61 / 100\tTraining Loss: 134.77700686454773\tTest Loss: 36.26178798079491\n",
      "Epoch: 62 / 100\tTraining Loss: 134.56750398874283\tTest Loss: 36.36930540204048\n",
      "Epoch: 63 / 100\tTraining Loss: 134.3830843269825\tTest Loss: 35.89995536208153\n",
      "Epoch: 64 / 100\tTraining Loss: 134.22210678458214\tTest Loss: 37.44350093603134\n",
      "Epoch: 65 / 100\tTraining Loss: 134.31491148471832\tTest Loss: 36.42756715416908\n",
      "Epoch: 66 / 100\tTraining Loss: 133.98335886001587\tTest Loss: 35.9828961789608\n",
      "Epoch: 67 / 100\tTraining Loss: 133.91022527217865\tTest Loss: 36.17883452773094\n",
      "Epoch: 68 / 100\tTraining Loss: 133.51893836259842\tTest Loss: 35.96130481362343\n",
      "Epoch: 69 / 100\tTraining Loss: 133.90603253245354\tTest Loss: 36.07605600357056\n",
      "Epoch: 70 / 100\tTraining Loss: 133.3829669356346\tTest Loss: 35.91865962743759\n",
      "Epoch: 71 / 100\tTraining Loss: 133.3340601027012\tTest Loss: 36.70209327340126\n",
      "Epoch: 72 / 100\tTraining Loss: 133.73578864336014\tTest Loss: 36.085397362709045\n",
      "Epoch: 73 / 100\tTraining Loss: 133.52470207214355\tTest Loss: 36.064752370119095\n",
      "Epoch: 74 / 100\tTraining Loss: 133.1799268424511\tTest Loss: 36.026156067848206\n",
      "Epoch: 75 / 100\tTraining Loss: 133.26336923241615\tTest Loss: 35.974625289440155\n",
      "Epoch: 76 / 100\tTraining Loss: 133.26395693421364\tTest Loss: 35.977458477020264\n",
      "Epoch: 77 / 100\tTraining Loss: 132.8995196223259\tTest Loss: 36.6982978284359\n",
      "Epoch: 78 / 100\tTraining Loss: 133.10581400990486\tTest Loss: 35.85940346121788\n",
      "Epoch: 79 / 100\tTraining Loss: 132.6943188905716\tTest Loss: 35.95867204666138\n",
      "Epoch: 80 / 100\tTraining Loss: 132.88827127218246\tTest Loss: 35.84911406040192\n",
      "Epoch: 81 / 100\tTraining Loss: 133.28380650281906\tTest Loss: 36.01386970281601\n",
      "Epoch: 82 / 100\tTraining Loss: 132.37537708878517\tTest Loss: 36.04409897327423\n",
      "Epoch: 83 / 100\tTraining Loss: 132.8774455487728\tTest Loss: 35.851144552230835\n",
      "Epoch: 84 / 100\tTraining Loss: 132.53932788968086\tTest Loss: 35.89164027571678\n",
      "Epoch: 85 / 100\tTraining Loss: 132.59623190760612\tTest Loss: 36.02489432692528\n",
      "Epoch: 86 / 100\tTraining Loss: 132.02949604392052\tTest Loss: 36.67078024148941\n",
      "Epoch: 87 / 100\tTraining Loss: 132.04837357997894\tTest Loss: 35.88687840104103\n",
      "Epoch: 88 / 100\tTraining Loss: 132.11943557858467\tTest Loss: 35.89442652463913\n",
      "Epoch: 89 / 100\tTraining Loss: 131.82287102937698\tTest Loss: 35.74980768561363\n",
      "Epoch: 90 / 100\tTraining Loss: 132.0360058248043\tTest Loss: 35.946353524923325\n",
      "Epoch: 91 / 100\tTraining Loss: 131.82789173722267\tTest Loss: 35.85142061114311\n",
      "Epoch: 92 / 100\tTraining Loss: 131.64694225788116\tTest Loss: 35.84598225355148\n",
      "Epoch: 93 / 100\tTraining Loss: 131.85038036108017\tTest Loss: 36.01533231139183\n",
      "Epoch: 94 / 100\tTraining Loss: 131.46432846784592\tTest Loss: 36.32975900173187\n",
      "Epoch: 95 / 100\tTraining Loss: 131.63746419548988\tTest Loss: 36.03915509581566\n",
      "Epoch: 96 / 100\tTraining Loss: 131.45140880346298\tTest Loss: 35.91527062654495\n",
      "Epoch: 97 / 100\tTraining Loss: 131.1892467737198\tTest Loss: 35.95221844315529\n",
      "Epoch: 98 / 100\tTraining Loss: 131.4272505044937\tTest Loss: 36.182045221328735\n",
      "Epoch: 99 / 100\tTraining Loss: 131.37129864096642\tTest Loss: 35.74391394853592\n",
      "Epoch: 100 / 100\tTraining Loss: 131.15149646997452\tTest Loss: 36.11794114112854\n"
     ]
    }
   ],
   "source": [
    "train(rnnModel, train_loader3, test_loader3, criterion, optimizer, n_epochs=100, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e1bd8ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.955\n"
     ]
    }
   ],
   "source": [
    "rnn_accuracy = accuracy(rnnModel, test_loader3, device)\n",
    "\n",
    "print(rnn_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1b10f",
   "metadata": {},
   "source": [
    "## Task 5(b): GRU\n",
    "\n",
    "* Here, we train a GRU on sentence embeddings obtained by stacking the first 10 word embeddings and applying padding if the sentence is smaller than 10 words.\n",
    "* We train the neural network using AdamW optimizer with 1e-3 learning rate with 1e-2 weight decay for 30 epochs with batch size 256.\n",
    "* Upon re-running the code several times, we observe the accuracy on test set for this model is roughly around **79.1% - 79.7%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6453a8c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUClf(\n",
      "  (gru): GRU(300, 10, batch_first=True)\n",
      "  (linear): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GRUClf(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUClf, self).__init__()\n",
    "        self.embed_size = 300\n",
    "    \n",
    "        self.hidden_size = 10\n",
    "        self.out_size = 2\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, hidden = self.gru(x)\n",
    "        x = self.linear(x[:, -1, :])\n",
    "        \n",
    "        return x\n",
    "    \n",
    "gruModel = GRUClf()\n",
    "gruModel.to(device)\n",
    "print(gruModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4973f4eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(gruModel.parameters(), lr=1e-3, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d71791c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 30\tTraining Loss: 181.33681312203407\tTest Loss: 40.29534995555878\n",
      "Epoch: 2 / 30\tTraining Loss: 152.9514371752739\tTest Loss: 38.26805377006531\n",
      "Epoch: 3 / 30\tTraining Loss: 146.8739361166954\tTest Loss: 37.00517484545708\n",
      "Epoch: 4 / 30\tTraining Loss: 143.26274248957634\tTest Loss: 36.36405465006828\n",
      "Epoch: 5 / 30\tTraining Loss: 141.10489463806152\tTest Loss: 35.926189661026\n",
      "Epoch: 6 / 30\tTraining Loss: 139.08646640181541\tTest Loss: 35.67848202586174\n",
      "Epoch: 7 / 30\tTraining Loss: 137.7913582623005\tTest Loss: 35.40585646033287\n",
      "Epoch: 8 / 30\tTraining Loss: 136.35473904013634\tTest Loss: 35.08504235744476\n",
      "Epoch: 9 / 30\tTraining Loss: 135.20484054088593\tTest Loss: 34.93747437000275\n",
      "Epoch: 10 / 30\tTraining Loss: 134.1550863981247\tTest Loss: 34.81758573651314\n",
      "Epoch: 11 / 30\tTraining Loss: 133.29672893881798\tTest Loss: 34.710423558950424\n",
      "Epoch: 12 / 30\tTraining Loss: 132.54534032940865\tTest Loss: 34.62228259444237\n",
      "Epoch: 13 / 30\tTraining Loss: 131.5119285285473\tTest Loss: 34.5435888171196\n",
      "Epoch: 14 / 30\tTraining Loss: 131.05372193455696\tTest Loss: 34.433901876211166\n",
      "Epoch: 15 / 30\tTraining Loss: 130.31678879261017\tTest Loss: 34.589472591876984\n",
      "Epoch: 16 / 30\tTraining Loss: 129.95355436205864\tTest Loss: 34.305437207221985\n",
      "Epoch: 17 / 30\tTraining Loss: 128.96858605742455\tTest Loss: 34.44904673099518\n",
      "Epoch: 18 / 30\tTraining Loss: 128.68122020363808\tTest Loss: 34.321634382009506\n",
      "Epoch: 19 / 30\tTraining Loss: 128.16218599677086\tTest Loss: 34.297405660152435\n",
      "Epoch: 20 / 30\tTraining Loss: 127.77931371331215\tTest Loss: 34.212056785821915\n",
      "Epoch: 21 / 30\tTraining Loss: 127.33910638093948\tTest Loss: 34.577800303697586\n",
      "Epoch: 22 / 30\tTraining Loss: 127.14896404743195\tTest Loss: 34.326654225587845\n",
      "Epoch: 23 / 30\tTraining Loss: 126.58092293143272\tTest Loss: 34.400550216436386\n",
      "Epoch: 24 / 30\tTraining Loss: 126.18402501940727\tTest Loss: 34.0765418112278\n",
      "Epoch: 25 / 30\tTraining Loss: 125.64939358830452\tTest Loss: 34.106071442365646\n",
      "Epoch: 26 / 30\tTraining Loss: 125.47544345259666\tTest Loss: 34.14732027053833\n",
      "Epoch: 27 / 30\tTraining Loss: 125.23535743355751\tTest Loss: 34.172821909189224\n",
      "Epoch: 28 / 30\tTraining Loss: 124.74122029542923\tTest Loss: 33.96618315577507\n",
      "Epoch: 29 / 30\tTraining Loss: 124.36532750725746\tTest Loss: 34.191488176584244\n",
      "Epoch: 30 / 30\tTraining Loss: 124.17429465055466\tTest Loss: 34.5347506403923\n"
     ]
    }
   ],
   "source": [
    "train(gruModel, train_loader3, test_loader3, criterion, optimizer, n_epochs=30, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ff9a728b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.275\n"
     ]
    }
   ],
   "source": [
    "gru_accuracy = accuracy(gruModel, test_loader3, device)\n",
    "\n",
    "print(gru_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ebb429",
   "metadata": {},
   "source": [
    "## Task 5(c): LSTM\n",
    "\n",
    "* Here, we train a LSTM on sentence embeddings obtained by stacking the first 10 word embeddings and applying padding if the sentence is smaller than 10 words.\n",
    "* We train the neural network using AdamW optimizer with 1e-3 learning rate and 1e-2 weight decay for 30 epochs with batch size 256.\n",
    "* A relu activation layer between the lstm cell and the linear layer is applied. Also, the output is passed through a tanh layer before being returned by the model.\n",
    "* Upon re-running the code several times, we observe the accuracy on test set for this model is roughly between **79.2% - 79.8%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de336160",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClf(\n",
      "  (lstm): LSTM(300, 10, batch_first=True)\n",
      "  (relu): ReLU()\n",
      "  (linear): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (tanh): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LSTMClf(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMClf, self).__init__()\n",
    "        self.embed_size = 300\n",
    "        self.hidden_size = 10\n",
    "        self.out_size = 2\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(self.hidden_size, self.out_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, (hidden, cell) = self.lstm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear(x[:, -1, :])\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "lstmModel = LSTMClf()\n",
    "lstmModel.to(device)\n",
    "print(lstmModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dce0b939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(lstmModel.parameters(), lr=1e-3, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cbc666e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 30\tTraining Loss: 188.53236678242683\tTest Loss: 42.60245108604431\n",
      "Epoch: 2 / 30\tTraining Loss: 161.78118962049484\tTest Loss: 40.11137869954109\n",
      "Epoch: 3 / 30\tTraining Loss: 154.7561023235321\tTest Loss: 39.04612699151039\n",
      "Epoch: 4 / 30\tTraining Loss: 150.59949985146523\tTest Loss: 38.261914163827896\n",
      "Epoch: 5 / 30\tTraining Loss: 147.96168661117554\tTest Loss: 37.8667289018631\n",
      "Epoch: 6 / 30\tTraining Loss: 145.9180660545826\tTest Loss: 37.60482919216156\n",
      "Epoch: 7 / 30\tTraining Loss: 144.42314419150352\tTest Loss: 37.16939628124237\n",
      "Epoch: 8 / 30\tTraining Loss: 142.6743516921997\tTest Loss: 36.983817517757416\n",
      "Epoch: 9 / 30\tTraining Loss: 141.31478962302208\tTest Loss: 36.718128740787506\n",
      "Epoch: 10 / 30\tTraining Loss: 140.29806298017502\tTest Loss: 36.9502349793911\n",
      "Epoch: 11 / 30\tTraining Loss: 139.1070382297039\tTest Loss: 36.48337149620056\n",
      "Epoch: 12 / 30\tTraining Loss: 138.49619647860527\tTest Loss: 36.24354547262192\n",
      "Epoch: 13 / 30\tTraining Loss: 137.58472111821175\tTest Loss: 36.23565372824669\n",
      "Epoch: 14 / 30\tTraining Loss: 136.69559437036514\tTest Loss: 36.28344339132309\n",
      "Epoch: 15 / 30\tTraining Loss: 136.19343376159668\tTest Loss: 36.044730961322784\n",
      "Epoch: 16 / 30\tTraining Loss: 135.28828984498978\tTest Loss: 36.178139090538025\n",
      "Epoch: 17 / 30\tTraining Loss: 134.70898419618607\tTest Loss: 35.870521783828735\n",
      "Epoch: 18 / 30\tTraining Loss: 133.9309034049511\tTest Loss: 35.881180971860886\n",
      "Epoch: 19 / 30\tTraining Loss: 133.92980736494064\tTest Loss: 35.94572842121124\n",
      "Epoch: 20 / 30\tTraining Loss: 132.89750257134438\tTest Loss: 35.94690823554993\n",
      "Epoch: 21 / 30\tTraining Loss: 132.76369643211365\tTest Loss: 35.764438807964325\n",
      "Epoch: 22 / 30\tTraining Loss: 131.8276786506176\tTest Loss: 35.75800174474716\n",
      "Epoch: 23 / 30\tTraining Loss: 131.56945458054543\tTest Loss: 36.201402485370636\n",
      "Epoch: 24 / 30\tTraining Loss: 131.06357857584953\tTest Loss: 35.585066854953766\n",
      "Epoch: 25 / 30\tTraining Loss: 130.93170562386513\tTest Loss: 35.64456853270531\n",
      "Epoch: 26 / 30\tTraining Loss: 130.27441161870956\tTest Loss: 36.144619435071945\n",
      "Epoch: 27 / 30\tTraining Loss: 129.86872991919518\tTest Loss: 35.68116870522499\n",
      "Epoch: 28 / 30\tTraining Loss: 129.67316290736198\tTest Loss: 35.78095597028732\n",
      "Epoch: 29 / 30\tTraining Loss: 129.42498436570168\tTest Loss: 35.652844071388245\n",
      "Epoch: 30 / 30\tTraining Loss: 128.7190609574318\tTest Loss: 35.79785969853401\n"
     ]
    }
   ],
   "source": [
    "train(lstmModel, train_loader3, test_loader3, criterion, optimizer, n_epochs=30, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22e0e5c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.56\n"
     ]
    }
   ],
   "source": [
    "lstm_accuracy = accuracy(lstmModel, test_loader3, device)\n",
    "\n",
    "print(lstm_accuracy) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
