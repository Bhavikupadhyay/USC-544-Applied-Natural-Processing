{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac0a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4371a4db",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "The goal of this task is to create a vocabulary. \n",
    "\n",
    "* To do this, we must first load the three files: train.json, dev.json, test.json from the data folder. Here, we have assumed that our program is available on the same level as the data folder. Hence, the path for the data folder is simply, './data'. However, it can be changed to point to any other location\n",
    "\n",
    "* Once the data is loaded, we create the vocabulary using the create_vocabulary helper function.\n",
    "* We also create mappers for both the words and tags, as they will be helpful in future tasks\n",
    "\n",
    "#### Observations\n",
    "* We use a threshold of 2 to create the vocabulary\n",
    "* There are totally 23183 unique words in the vocabulary (including the special token \"\\<unk>\", with each word having three values representing the word, index and frequency of appearance in training data\n",
    "* The special token \"\\<unk>\" appears 20011 times in the vocabulary\n",
    "\n",
    "#### Note: All the outputs are stored in the folder called './out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc18945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply change the path below to reach the 'data' folder and load the data.\n",
    "# Ensure the 'data' folder contains the three json files.\n",
    "data_path = './data' \n",
    "\n",
    "with open(os.path.join(data_path, 'train.json')) as train_file:\n",
    "    train_data = json.load(train_file)\n",
    "\n",
    "with open(os.path.join(data_path, 'dev.json')) as dev_file:\n",
    "    dev_data = json.load(dev_file)\n",
    "\n",
    "with open(os.path.join(data_path,'test.json')) as test_file:\n",
    "    test_data = json.load(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f819c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data, threshold=2, return_counter=False):\n",
    "    words_counter = Counter()\n",
    "    tags_counter = Counter()\n",
    "\n",
    "    for datum in data:\n",
    "        words_counter.update(datum['sentence'])\n",
    "        tags_counter.update(datum['labels'])\n",
    "\n",
    "    unk, count_unk = '<unk>', 0\n",
    "    for word, count in words_counter.items():\n",
    "        if count < threshold:\n",
    "            count_unk += count\n",
    "\n",
    "    words_counter = {key: value for key, value in words_counter.items() if value >= threshold}\n",
    "\n",
    "    words_counter = {key: value for key, value in\n",
    "                     sorted(words_counter.items(), key=lambda datum: datum[1], reverse=True)}\n",
    "    tags_counter = {key: value for key, value in sorted(tags_counter.items(), key=lambda datum: datum[1], reverse=True)}\n",
    "\n",
    "    vocab_list, tags_list = [], []\n",
    "    vocab_list.append([unk, 0, count_unk])\n",
    "\n",
    "    ind = 1\n",
    "    for word, count in words_counter.items():\n",
    "        vocab_list.append([word, ind, count])\n",
    "        ind += 1\n",
    "\n",
    "    ind = 0\n",
    "    for tag, count in tags_counter.items():\n",
    "        tags_list.append([tag, ind, count])\n",
    "        ind += 1\n",
    "\n",
    "    if return_counter:\n",
    "        return vocab_list, tags_list, tags_counter, words_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "733ab0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mapper(item_list):\n",
    "    item2ind = {datum[0]: datum[1] for datum in item_list}\n",
    "    ind2item = {datum[1]: datum[0] for datum in item_list}\n",
    "\n",
    "    return item2ind, ind2item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "603cbb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list, tags_list, tags_counter, _ = create_vocabulary(train_data, return_counter=True)\n",
    "word2ind, ind2word = create_mapper(vocab_list)\n",
    "tag2ind, ind2tag = create_mapper(tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a98676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is: 23183\n"
     ]
    }
   ],
   "source": [
    "print('The length of the vocabulary is:', len(vocab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55e30c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The special token '<unk>' appears 20011 times in the training data following the replacement process\n"
     ]
    }
   ],
   "source": [
    "print('The special token \\'<unk>\\' appears', vocab_list[0][2], 'times in the training data following the replacement process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6808e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ./out folder\n",
    "if not os.path.exists('./out'):\n",
    "    os.makedirs('./out')\n",
    "\n",
    "# Writing the vocabulary to vocab.txt\n",
    "with open('./out/vocab.txt', 'w') as vocab_file:\n",
    "    for datum in vocab_list:\n",
    "        vocab_file.write(f'{datum[0]}\\t{datum[1]}\\t{datum[2]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc50a5b",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "The goal of this task is create a HMM model and learn the parameters from training data.\n",
    "\n",
    "* For this purpose, I have created a class called HMM, with two methods for learning: from training data and from json file (loading the weights).\n",
    "\n",
    "* This class also consists of additional methods for viterbi decoding and greedy decoding\n",
    "\n",
    "#### Observations\n",
    "* The number of parameters in transition matrix in the HMM: 2025\n",
    "* The number of parameters in emission matrix in the HMM: 1043235\n",
    "* The number of parameters in the initial state matrix (pi): 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c77e2dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, tags_counter, word2ind, ind2word, tag2ind, ind2tag):\n",
    "        self.transition_matrix = None\n",
    "        self.emission_matrix = None\n",
    "        self.pi_matrix = None\n",
    "        self.word2ind = word2ind\n",
    "        self.ind2word = ind2word\n",
    "        self.tag2ind = tag2ind\n",
    "        self.ind2tag = ind2tag\n",
    "\n",
    "        self.tags_counter = tags_counter\n",
    "\n",
    "        self.num_tags = len(self.tag2ind)\n",
    "        self.num_words = len(self.word2ind)\n",
    "\n",
    "    def create_hmm_from_data(self, data, fill_value=1e-6):\n",
    "        transition_dict = self.create_transition_dict(data)\n",
    "        emission_dict = self.create_emission_dict(data)\n",
    "        pi_dict = self.create_pi_dict(data)\n",
    "\n",
    "        self.transition_matrix = np.full(shape=(self.num_tags, self.num_tags), fill_value=fill_value, dtype='float')\n",
    "        self.emission_matrix = np.full(shape=(self.num_tags, self.num_words), fill_value=fill_value, dtype='float')\n",
    "        self.pi_matrix = np.full(shape=self.num_tags, fill_value=fill_value, dtype='float')\n",
    "\n",
    "        for (s, s_prime), prob in transition_dict.items():\n",
    "            self.transition_matrix[self.tag2ind[s], self.tag2ind[s_prime]] = prob\n",
    "\n",
    "        for (s, x), prob in emission_dict.items():\n",
    "            if x in self.word2ind.keys():\n",
    "                self.emission_matrix[self.tag2ind[s], self.word2ind[x]] = prob\n",
    "            else:\n",
    "                self.emission_matrix[self.tag2ind[x], self.word2ind[unk]] = prob\n",
    "\n",
    "        for s, prob in pi_dict.items():\n",
    "            self.pi_matrix[self.tag2ind[s]] = prob\n",
    "\n",
    "    def create_hmm_from_json(self, hmm_file):\n",
    "        with open(hmm_file) as hmm_file:\n",
    "            hmm_data = json.load(hmm_file)\n",
    "\n",
    "        self.transition_matrix = np.array(hmm_data['transition'])\n",
    "        self.emission_matrix = np.array(hmm_data['emission'])\n",
    "        self.pi_matrix = np.array(hmm_data['pi'])\n",
    "\n",
    "    def create_transition_dict(self, data):\n",
    "        transition_probs = defaultdict(float)\n",
    "\n",
    "        for datum in data:\n",
    "            num_labels = len(datum['labels'])\n",
    "            for i in range(num_labels - 1):\n",
    "                s = datum['labels'][i]\n",
    "                s_prime = datum['labels'][i + 1]\n",
    "                transition = (s, s_prime)\n",
    "\n",
    "                transition_probs[transition] += 1. / self.tags_counter[s]\n",
    "\n",
    "        return transition_probs\n",
    "\n",
    "    def create_emission_dict(self, data, unk='<unk>'):\n",
    "        emission_probs = defaultdict(float)\n",
    "\n",
    "        for datum in data:\n",
    "            num_words = len(datum['sentence'])\n",
    "\n",
    "            for i in range(num_words):\n",
    "                word = datum['sentence'][i]\n",
    "                x = word if word in self.word2ind.keys() else unk\n",
    "                s = datum['labels'][i]\n",
    "                emission = (s, x)\n",
    "\n",
    "                emission_probs[emission] += 1. / self.tags_counter[s]\n",
    "\n",
    "        return emission_probs\n",
    "\n",
    "    def create_pi_dict(self, data):\n",
    "        pi_probs = defaultdict(int)\n",
    "\n",
    "        for datum in data:\n",
    "            s = datum['labels'][0]\n",
    "            pi_probs[s] += 1. / len(data)\n",
    "\n",
    "        return pi_probs\n",
    "\n",
    "    def greedy_decode(self, sentence, unk='<unk>'):\n",
    "        words = np.array([self.word2ind[word] if word in word2ind.keys() else word2ind[unk] for word in sentence])\n",
    "\n",
    "        y_preds = np.zeros(shape=len(words))\n",
    "\n",
    "        y_prev = np.argmax(self.pi_matrix * self.emission_matrix[:, words[0]])\n",
    "\n",
    "        y_preds[0] = y_prev\n",
    "\n",
    "        for i in range(1, len(words)):\n",
    "            word = words[i]\n",
    "            y_prev = np.argmax(self.transition_matrix[y_prev, :] * self.emission_matrix[:, word])\n",
    "            y_preds[i] = y_prev\n",
    "\n",
    "        Y = np.array([self.ind2tag[ind] for ind in y_preds])\n",
    "        return Y\n",
    "\n",
    "    def viterbi_decode(self, sentence, unk='<unk>'):\n",
    "        words = np.array([self.word2ind[word] if word in word2ind.keys() else word2ind[unk] for word in sentence])\n",
    "        num_words = len(words)\n",
    "        T1 = np.zeros(shape=(self.num_tags, num_words), dtype='float')\n",
    "        T2 = np.zeros(shape=(self.num_tags, num_words), dtype='int')\n",
    "\n",
    "        for state in range(self.num_tags):\n",
    "            T1[state, 0] = self.pi_matrix[state] * self.emission_matrix[state, words[0]]\n",
    "\n",
    "        for obs in range(1, num_words):\n",
    "            for state in range(self.num_tags):\n",
    "                word = words[obs]\n",
    "\n",
    "                k = np.argmax(T1[:, obs-1] * self.transition_matrix[:, state] * self.emission_matrix[state, word])\n",
    "                T2[state, obs] = k\n",
    "                T1[state, obs] = T1[k, obs-1] * self.transition_matrix[k, state] * self.emission_matrix[state, word]\n",
    "\n",
    "        best_path = []\n",
    "        k = np.argmax(T1[:, num_words-1])\n",
    "        for obs in reversed(range(num_words)):\n",
    "            best_path.insert(0, k)\n",
    "            k = T2[k, obs]\n",
    "\n",
    "        Y = np.array([self.ind2tag[ind] for ind in best_path])\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "899e1efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HMM(tags_counter, word2ind, ind2word, tag2ind, ind2tag)\n",
    "hmm.create_hmm_from_data(train_data, fill_value=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "995a486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transition parameters: 45*45 = 2025\n",
      "Number of emission parameters: 45*23183 = 1043235\n",
      "Number of parameters in the initial state matrix (pi): 45\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of transition parameters: {len(hmm.transition_matrix)}*{len(hmm.transition_matrix[0,])} = {len(hmm.transition_matrix) * len(hmm.transition_matrix[0])}')\n",
    "print(f'Number of emission parameters: {len(hmm.emission_matrix)}*{len(hmm.emission_matrix[0])} = {len(hmm.emission_matrix) * len(hmm.emission_matrix[0])}')\n",
    "print(f'Number of parameters in the initial state matrix (pi): {len(hmm.pi_matrix)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66a90a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./out/hmm.json', 'w') as hmm_file:\n",
    "    json.dump({\n",
    "        'transition': hmm.transition_matrix.tolist(),\n",
    "        'emission': hmm.emission_matrix.tolist(),\n",
    "        'pi': hmm.pi_matrix.tolist(),\n",
    "    }, hmm_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6342c8b5",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "The next task is to perform greedy decoding. As, I have created a class for HMM, the method for greedy decoding is already written inside the class under the method 'greedy_decode'. \n",
    "So, using the created hmm, only the data needs to be passed to the greedy_decode function.\n",
    "\n",
    "* A helper function for calculating accuracy from a list of list of strings (tags) is created here\n",
    "* Here, we perform greedy decoding on dev data and compare the generated labels with the labels present in the dev data and obtain the accuracy\n",
    "* We also generate the labels for test data and store them in a file called 'greedy.json'\n",
    "\n",
    "#### Observations:\n",
    "* We obtain 93.5113% accuracy on the dev data using greedy decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89647e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_preds):\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for true_sentence, pred_sentence in zip(y_true, y_preds):\n",
    "        correct += sum(true_sentence == pred_sentence)\n",
    "        total += len(true_sentence)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f381fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = [datum['sentence'] for datum in dev_data]\n",
    "Y_true_dev = [datum['labels'] for datum in dev_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21ab995f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for greedy decoding on dev data: 0.9351132293121244\n"
     ]
    }
   ],
   "source": [
    "Y_preds_greedy = [hmm.greedy_decode(sentence) for sentence in X_dev]\n",
    "greedy_acc = accuracy(Y_true_dev, Y_preds_greedy)\n",
    "\n",
    "print('Accuracy for greedy decoding on dev data:', greedy_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e704514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_results = []\n",
    "for datum in test_data:\n",
    "    sentence = datum['sentence']\n",
    "    index = datum['index']\n",
    "    greedy_labels = hmm.greedy_decode(sentence)\n",
    "\n",
    "    greedy_datum = {'index': index, 'sentence': sentence, 'labels': greedy_labels.tolist()}\n",
    "    greedy_results.append(greedy_datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f286578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./out/greedy.json', 'w') as greedy_file:\n",
    "    json.dump(greedy_results, greedy_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2542d3",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "Just like in task 3, since we have a hmm object created, we only need to run the method 'viterbi_decode' on the data.\n",
    "\n",
    "* We first run the method on the sentences in dev data and obtain the accuracy by comparision with corresponding labels\n",
    "* We then create viterbi.json by predicting the tags for all the sentences in the test data\n",
    "\n",
    "#### Observations\n",
    "* We obtain 94.8158% accuracy using viterbi decoding on dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9943f1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for viterbi decoding on dev data: 0.9481588853135815\n"
     ]
    }
   ],
   "source": [
    "Y_preds_viterbi = [hmm.viterbi_decode(sentence) for sentence in X_dev]\n",
    "viterbi_acc = accuracy(Y_true_dev, Y_preds_viterbi)\n",
    "\n",
    "print('Accuracy for viterbi decoding on dev data:', viterbi_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42bd12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_results = []\n",
    "for datum in test_data:\n",
    "    sentence = datum['sentence']\n",
    "    index = datum['index']\n",
    "    viterbi_labels = hmm.viterbi_decode(sentence)\n",
    "\n",
    "    viterbi_datum = {'index': index, 'sentence': sentence, 'labels': viterbi_labels.tolist()}\n",
    "    viterbi_results.append(viterbi_datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1b74997",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./out/viterbi.json', 'w') as viterbi_file:\n",
    "    json.dump(viterbi_results, viterbi_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d4e62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
