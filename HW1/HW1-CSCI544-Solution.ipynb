{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "The following libraries might be required for the .py file to function properly\n",
    "\n",
    "* scikit-learn\n",
    "* nltk\n",
    "* contractions\n",
    "* pandas\n",
    "* numpy\n",
    "* bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: bs4 in c:\\users\\bhavi\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\bhavi\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\bhavi\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: contractions in c:\\users\\bhavi\\anaconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\bhavi\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\bhavi\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "Requirement already satisfied: anyascii in c:\\users\\bhavi\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4\n",
    "! pip install contractions\n",
    "# Dataset: https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bhavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bhavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bhavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\bhavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os # for checking if file is present \n",
    "from urllib import request # for downloading the dataset\n",
    "import gzip # for extracting the dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk # for pre-processing tasks like tokenization, stop words removal, and lemmatization\n",
    "import re # for removing urls, extra spaces etc.\n",
    "from bs4 import BeautifulSoup # for removal of html\n",
    "import contractions # for expanding contractions\n",
    "\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk import pos_tag # pos_tagging to be used in conjunction with lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer # lemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # for creating bag-of-words and tf-idf dataset\n",
    "from sklearn.model_selection import train_test_split # for splitting into training and test sets\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score  # for calculating metrics\n",
    "\n",
    "# models to be used for training\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# downloading the different requirements for using nltk pos_tag, stop words and wordnet lemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "* First, we read the data. To do this, we make use of urllib.request library. We retrieve the file from the dataset url provided and then store it locally.\n",
    "* Once the data is downloaded, we extract it from the gzipped file and save a .tsv version.\n",
    "* This data can be then read using pd.read_csv or pd.read_table. \n",
    "* We use ‘\\t’ as the separator as it is a .tsv file. \n",
    "* While trying to create the data frame, there were errors where we had 21 columns instead of 15, so on_bad_lines was set to ‘skip’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
      "0          US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
      "1          US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
      "2          US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
      "3          US     52782374  R1PR37BR7G3M6A  B00D7H8XB6       868449945   \n",
      "4          US     24045652  R3BDDDZMZBZDPU  B001XCWP34        33521401   \n",
      "\n",
      "                                       product_title product_category  \\\n",
      "0     Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
      "1          Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
      "2  Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
      "3  AmazonBasics 12-Sheet High-Security Micro-Cut ...  Office Products   \n",
      "4  Derwent Colored Pencils, Inktense Ink Pencils,...  Office Products   \n",
      "\n",
      "  star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
      "0           5            0.0          0.0    N                 Y   \n",
      "1           5            0.0          1.0    N                 Y   \n",
      "2           5            0.0          0.0    N                 Y   \n",
      "3           1            2.0          3.0    N                 Y   \n",
      "4           4            0.0          0.0    N                 Y   \n",
      "\n",
      "                                     review_headline  \\\n",
      "0                                         Five Stars   \n",
      "1  Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
      "2                      but I am sure I will like it.   \n",
      "3  and the shredder was dirty and the bin was par...   \n",
      "4                                         Four Stars   \n",
      "\n",
      "                                         review_body review_date  \n",
      "0                                     Great product.  2015-08-31  \n",
      "1  What's to say about this commodity item except...  2015-08-31  \n",
      "2    Haven't used yet, but I am sure I will like it.  2015-08-31  \n",
      "3  Although this was labeled as &#34;new&#34; the...  2015-08-31  \n",
      "4                    Gorgeous colors and easy to use  2015-08-31  \n"
     ]
    }
   ],
   "source": [
    "url = 'https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz'\n",
    "\n",
    "extracted_file = 'amazon_reviews_us_Office_Products.tsv'\n",
    "compressed_file = extracted_file + '.gz'\n",
    "\n",
    "\n",
    "# Retrieve the dataset from given url and store it in location specified by compressed_file\n",
    "if not os.path.exists(extracted_file):\n",
    "    request.urlretrieve(url, compressed_file)\n",
    "\n",
    "    # extract the dataset from the gzipped file\n",
    "    with gzip.open(compressed_file, 'rb') as f_in, open(extracted_file, 'wb') as f_out:\n",
    "        for line in f_in:\n",
    "            f_out.write(line)\n",
    "\n",
    "    os.remove(compressed_file)\n",
    "    \n",
    "# read the extracted data into pandas dataframe\n",
    "original_df = pd.read_csv(extracted_file, sep='\\t', on_bad_lines='skip', low_memory=False)\n",
    "print(original_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings\n",
    "\n",
    "* Now, we try to save only two columns: review_body and star_rating.\n",
    "* Here, I noticed that some of the values in star_rating included dates, which was unexpected. \n",
    "* Since these were erroneous, I decided to drop them by converting the column to numeric and coercing any errors, which will turn them to NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         review_body star_rating\n",
      "0                                     Great product.           5\n",
      "1  What's to say about this commodity item except...           5\n",
      "2    Haven't used yet, but I am sure I will like it.           5\n",
      "3  Although this was labeled as &#34;new&#34; the...           1\n",
      "4                    Gorgeous colors and easy to use           4\n",
      "['5' '1' '4' '2' '3' '2015-06-05' '2015-02-11' nan '2014-02-14']\n",
      "[5. 1. 4. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "# creating the dataframe by taking only review_body and star_rating columns\n",
    "df = pd.DataFrame(original_df[['review_body', 'star_rating']])\n",
    "print(df.head())\n",
    "\n",
    "# we notice there are some erroneous values for the star_rating column\n",
    "print(df['star_rating'].unique())\n",
    "\n",
    "# converting the star_rating to numeric values and dropping erroneous columns\n",
    "df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(df['star_rating'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We form two classes and select 50000 reviews randomly from each class.\n",
    "\n",
    "* Now, a new column called target is created, where there are only two values: 1 and 2. 1 is given to star_rating rows with values 1, 2 or 3, and 2 is given to star_rating rows with values 4 or 5.\n",
    "* Afterwards, 50000 rows of each target class 1 or 2 are sampled into two different intermediate variables: class_1 and class_2.\n",
    "* Finally, a new dataframe is created concatenating these two intermediate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the target column: target = 1 if star_rating is 1, 2 or 3. target = 2 if star_rating is 4 or 5\n",
    "df['star_rating'] = df['star_rating'].astype(int)\n",
    "df['target'] = df['star_rating'].apply(lambda x: 1 if x <= 3 else 2)\n",
    "\n",
    "sample_size = 50000\n",
    "\n",
    "# creating a sample dataframe where target = 1 of size 50000 rows\n",
    "class_1 = df.loc[df['target'] == 1].sample(n=sample_size, random_state=42)\n",
    "\n",
    "# creating a sample dataframe where target = 2 of size 50000 rows\n",
    "class_2 = df.loc[df['target'] == 2].sample(n=sample_size, random_state=42)\n",
    "\n",
    "# merging the two sample dataframes\n",
    "df_new = pd.concat([class_1, class_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "* In cleaning of the data, we perform the following steps inside the clean() function:\n",
    "    * Converting to lower case: we use the string’s lower() method\n",
    "    * Removing html: Beautiful soup is used to perform this task. We use the decompose() method to remove any anchor tags which will contain html\n",
    "    * Removing urls: Urls are removed using regular expressions. This works for both http and https urls.\n",
    "    * Removing non-alphabetical characters: Non-alphabetical characters are removed by using regular expressions as well.\n",
    "    * Removal of extra spaces: Extra spaces can be removed by substituting any multiple spaces denoted by ‘\\s+’ with a single space.\n",
    "    * Expanding contractions: The contractions library is used to expand any contractions found within a review. We can use the fix() method to perform the expansion.\n",
    "* Before performing this data cleaning, the average character length of each review is 314.24925 and after data cleaning, it decreases to 298.3743.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhavi\\AppData\\Local\\Temp\\ipykernel_4476\\2550330846.py:15: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(review, \"html.parser\")\n",
      "C:\\Users\\bhavi\\AppData\\Local\\Temp\\ipykernel_4476\\2550330846.py:15: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(review, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of reviews before Cleaning: 314.24925, Average length of reviews after cleaning: 298.3743\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import contractions\n",
    "\n",
    "# convert to lower-case\n",
    "# remove html and urls\n",
    "# remove non-alphabetical character\n",
    "# remove extra spaces\n",
    "# perform contractions\n",
    "\n",
    "def clean(review):\n",
    "    # converting to lowercase\n",
    "    review = review.lower()\n",
    "    \n",
    "    # removing htmls\n",
    "    soup = BeautifulSoup(review, \"html.parser\")\n",
    "    \n",
    "    for a_tag in soup.find_all(\"a\"):\n",
    "        a_tag.decompose()\n",
    "        \n",
    "    review = soup.get_text()\n",
    "    \n",
    "    # removing urls\n",
    "    review = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', review)\n",
    "    \n",
    "    # removing non-alphabetical characters\n",
    "    review = re.sub(r'[^a-zA-Z\\s]', '', review)\n",
    "    \n",
    "    # removing extra spaces\n",
    "    review = re.sub(r'\\s+', ' ', review).strip()\n",
    "    \n",
    "    # expanding contractions\n",
    "    review = contractions.fix(review)\n",
    "    \n",
    "    return review\n",
    "    \n",
    "# calculating average character length of each review before cleaning\n",
    "before_cleaning = df_new['review_body'].apply(len).mean()\n",
    "\n",
    "df_new['review_body'] = df_new['review_body'].apply(clean)\n",
    "\n",
    "# calculating average character length of each review after cleaning\n",
    "after_cleaning = df_new['review_body'].apply(len).mean()\n",
    "\n",
    "print('Average length of reviews before Cleaning: ', before_cleaning, ', Average length of reviews after cleaning: ', after_cleaning, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words \n",
    "\n",
    "* We use nltk to remove stop words\n",
    "* We can obtain the set of stop words in english language from nltk.corpus.\n",
    "* First, we tokenize the words in the review and the token is only included in the output to be returned if it is not present in the set of stopwords.\n",
    "* In this way, we obtain all the words which are not in stop words\n",
    "* Before stop words removal we have approximately 298 characters per review which decreases to 188.39753 characters per review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of reviews before removing stop words: 298.3743 Average length of reviews after removing stop words: 188.39753\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(review):\n",
    "    # tokenizing words from the review\n",
    "    words = nltk.word_tokenize(review)\n",
    "    \n",
    "    # obtaining the set of stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "    \n",
    "    # not picking the word if it is present in the set of stop words\n",
    "    words = [word for word in words if word not in stop]\n",
    "    review = ' '.join(words)\n",
    "    \n",
    "    return review\n",
    "\n",
    "\n",
    "# average character length of each review before removing stop words\n",
    "before_stop_words = df_new['review_body'].apply(len).mean()\n",
    "\n",
    "df_new['review_body'] = df_new['review_body'].apply(remove_stopwords)\n",
    "\n",
    "after_stop_words = df_new['review_body'].apply(len).mean()\n",
    "\n",
    "print('Average length of reviews before removing stop words: ', before_stop_words, ' Average length of reviews after removing stop words: ', after_stop_words, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  \n",
    "\n",
    "* To perform lemmatization, we can use WordNetLemmatizer from nltk.stem.\n",
    "* However, it lemmatizes a word based on its part-of-speech which by default is considered as Noun.\n",
    "* In order to make the lemmatization more accurate, we have to provide its pos tag. We can do this by using the pos_tag() method from nltk.\n",
    "* However, this provides treebank tags, which need to be converted to Word Net compatible tags. \n",
    "* This conversion is done by first getting the treebank tags inside the lemmatize function. Then, we call the get_tag() function which converts a treebank tag to wordnet tag. The tag conversion is as follows:\n",
    "    * A treebank tag beginning with ‘J’ is an adjective\n",
    "    * A treebank tag beginning with ‘V’ is a verb\n",
    "    * A treebank tag beginning with ‘N’ is a noun\n",
    "    * A treebank tag beginning with ‘R’ is a adverb\n",
    "* After lemmatization, we notice the average character length drop further to 185.27033 characters per review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of characters before lemmatization: \t188.39753\tAverage length of characters after lemmatization: \t185.27033\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def lemmatize(review):\n",
    "    # tokenizing a review\n",
    "    words = nltk.word_tokenize(review)\n",
    "    \n",
    "    # creating tags for the review by obtaining the treebank tags and then converting to wordnet-compatible tags\n",
    "    treebank_tags = pos_tag(words)\n",
    "    tags = [get_tag(word) for word in words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # We lemmatize the words along with the tag if it is available, else use the default pos_tag used by the wordnet lemmatizer\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, tag) if tag != '' else lemmatizer.lemmatize(word) for (word, tag) in zip(words, tags)]\n",
    "    review = ' '.join(lemmatized_words)\n",
    "    \n",
    "    return review\n",
    "\n",
    "\n",
    "# average length of characters per review before lemmatization\n",
    "before_lemm = df_new['review_body'].apply(len).mean()\n",
    "\n",
    "df_new['review_body'] = df_new['review_body'].apply(lemmatize)\n",
    "\n",
    "# average length of characters per review after lemmatization\n",
    "after_lemm = df_new['review_body'].apply(len).mean()\n",
    "\n",
    "print('Average length of characters before lemmatization: ', before_lemm, 'Average length of characters after lemmatization: ', after_lemm, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of reviews before pre-processing: \t298.3743\t ,Average length of reviews after pre-processing: \t185.27033\n"
     ]
    }
   ],
   "source": [
    "print('Average length of reviews before pre-processing: ', before_stop_words, ' ,Average length of reviews after pre-processing: ', after_lemm, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF and BoW Feature Extraction\n",
    "\n",
    "* The next task is to extract TF-IDF and Bag-of-Words features.\n",
    "* We can use sklearn’s CountVectorizer (for bow) and TfidfVectorizer (for tf-idf) classes present in feature_extraction.text library in sklearn.\n",
    "* We use the fit_transform methods of both classes to obtain the numerical features\n",
    "* We also split the tf-idf matrix, bow matrix and the target column in df_new in a single step using train_test_split function from sklearn.model_selection. This will help maintain correspondence between not only tf-idf matrix and target, and bow and target, but also tf-idf and bow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag-of-Words dataset\n",
    "bow_extractor = CountVectorizer()\n",
    "bow_matrix = bow_extractor.fit_transform(df_new['review_body'])\n",
    "\n",
    "# Creating the TF-IDF Dataset\n",
    "tf_idf_extractor = TfidfVectorizer()\n",
    "tf_idf_matrix = tf_idf_extractor.fit_transform(df_new['review_body'])\n",
    "\n",
    "# creating the train and test sets for Bag-of-Words, TF-IDF and targets column\n",
    "bow_X_train, bow_X_test, tf_idf_X_train, tf_idf_X_test, Y_train, Y_test = train_test_split(bow_matrix, tf_idf_matrix, df_new['target'], test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Using Both Features\n",
    "\n",
    "Performance for perceptron for bag of words:\n",
    "* Precision: 0.8156452416542103 \n",
    "* Recall: 0.7908212560386474 \n",
    "* F1-Score: 0.8030414520480745\n",
    "\n",
    "Performance for perceptron for tf-idf:\n",
    "* Precision: 0.8338814150473344 \n",
    "* Recall: 0.7725258493353028 \n",
    "* F1-Score: 0.8020319164230604"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW:  0.7908212560386474 0.8156452416542103 0.8030414520480745\n",
      "TF-IDF:  0.7725258493353028 0.8338814150473344 0.8020319164230604\n"
     ]
    }
   ],
   "source": [
    "# Creating and training the perceptron for bag-of-words\n",
    "bow_clf = Perceptron(penalty='elasticnet', l1_ratio=0.1, eta0=1e-3, alpha=1e-6, tol=1e-4, random_state=42)\n",
    "bow_clf.fit(bow_X_train, Y_train)\n",
    "\n",
    "# making predictions on the bag-of-words test set\n",
    "bow_Y_pred = bow_clf.predict(bow_X_test)\n",
    "\n",
    "# calculating and printing the precision, recall and f1 scores for perceptron on the bag-of-words test set\n",
    "bow_precision = precision_score(Y_test, bow_Y_pred)\n",
    "bow_recall = recall_score(Y_test, bow_Y_pred)\n",
    "bow_f1 = f1_score(Y_test, bow_Y_pred)\n",
    "print('BOW: ', bow_precision, bow_recall, bow_f1)\n",
    "\n",
    "# Creating and training the perceptron for TF-IDF\n",
    "tf_idf_clf = Perceptron(penalty='elasticnet', l1_ratio=0.3, eta0=1e-5, max_iter=1000, alpha=1e-6, tol=1e-4, random_state=42)\n",
    "tf_idf_clf.fit(tf_idf_X_train, Y_train)\n",
    "\n",
    "# making predictions on the TD-IDF test set\n",
    "tf_idf_Y_pred = tf_idf_clf.predict(tf_idf_X_test)\n",
    "\n",
    "# calculating and printing the Precision, Recall and F1 Scores for perceptron on the TF-IDF test set\n",
    "tf_idf_precision = precision_score(Y_test, tf_idf_Y_pred)\n",
    "tf_idf_recall = recall_score(Y_test, tf_idf_Y_pred)\n",
    "tf_idf_f1 = f1_score(Y_test, tf_idf_Y_pred)\n",
    "\n",
    "\n",
    "print('TF-IDF: ', tf_idf_precision, tf_idf_recall, tf_idf_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Using Both Features\n",
    "\n",
    "Performance for SVM for bag of words:\n",
    "* Precision: 0.820627802690583 \n",
    "* Recall: 0.8598726114649682 \n",
    "* F1-Score: 0.8397919641036101\n",
    "\n",
    "Performance for SVM for tf-idf:\n",
    "* Precision: 0.8589935226706528 \n",
    "* Recall: 0.8424550430023455 \n",
    "* F1-Score: 0.8506439038831598"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW:  0.8598726114649682 0.820627802690583 0.8397919641036101\n",
      "TF-IDF:  0.8424550430023455 0.8589935226706528 0.8506439038831598\n"
     ]
    }
   ],
   "source": [
    "# Creating and training SVM model on Bag-of-words training set\n",
    "bow_svm = LinearSVC(max_iter=1000, penalty='l1', dual=False, C=0.1, random_state=42)\n",
    "bow_svm.fit(bow_X_train, Y_train)\n",
    "\n",
    "# making predictions on bag-of-words test set\n",
    "bow_Y_pred = bow_svm.predict(bow_X_test)\n",
    "\n",
    "# calculating and printing the precision, recall and f1-scores for svm on bag-of-words test set\n",
    "bow_precision = precision_score(Y_test, bow_Y_pred)\n",
    "bow_recall = recall_score(Y_test, bow_Y_pred)\n",
    "bow_f1 = f1_score(Y_test, bow_Y_pred)\n",
    "print('BOW: ', bow_precision, bow_recall, bow_f1)\n",
    "\n",
    "\n",
    "# Creating and training SVM model on TFIDF training set\n",
    "tf_idf_svm = LinearSVC(max_iter=10, dual=False, C=0.1, random_state=42)\n",
    "tf_idf_svm.fit(tf_idf_X_train, Y_train)\n",
    "\n",
    "# making predictions on TF-IDF test set\n",
    "tf_idf_Y_pred = tf_idf_svm.predict(tf_idf_X_test)\n",
    "\n",
    "# calculating and printing the precision, recall and f1-scores for svm on tf-idf test set\n",
    "tf_idf_precision = precision_score(Y_test, tf_idf_Y_pred)\n",
    "tf_idf_recall = recall_score(Y_test, tf_idf_Y_pred)\n",
    "tf_idf_f1 = f1_score(Y_test, tf_idf_Y_pred)\n",
    "print('TF-IDF: ', tf_idf_precision, tf_idf_recall, tf_idf_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Using Both Features\n",
    "\n",
    "Performance for logistic regression for bag of words:\n",
    "* Precision: 0.8260089686098655 \n",
    "* Recall: 0.8555062441944473 \n",
    "* F1-Score: 0.8404988846075846\n",
    "\n",
    "Performance for logistic regression for tf-idf:\n",
    "* Precision: 0.8579970104633782 \n",
    "* Recall: 0.8422185268512179 \n",
    "* F1-Score: 0.8500345542501728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW:  0.8555062441944473 0.8260089686098655 0.8404988846075846\n",
      "TF-IDF:  0.8422185268512179 0.8579970104633782 0.8500345542501728\n"
     ]
    }
   ],
   "source": [
    "# Creating and training a Logistic Regression model on Bag-of-words training set\n",
    "bow_log_reg = LogisticRegression(max_iter=1000, C=0.3, random_state=42)\n",
    "bow_log_reg.fit(bow_X_train, Y_train)\n",
    "\n",
    "# making predictions on bag-of-words test set\n",
    "bow_Y_pred = bow_log_reg.predict(bow_X_test)\n",
    "\n",
    "# calculating and printing precision, recall and f1-scores for logistic regression on bag-of-words test set\n",
    "bow_precision = precision_score(Y_test, bow_Y_pred)\n",
    "bow_recall = recall_score(Y_test, bow_Y_pred)\n",
    "bow_f1 = f1_score(Y_test, bow_Y_pred)\n",
    "print('BOW: ', bow_precision, bow_recall, bow_f1)\n",
    "\n",
    "\n",
    "# Creating and training a Logistic Regression model on TF-IDF training set\n",
    "tf_idf_log_reg = LogisticRegression(max_iter=200, random_state=42)\n",
    "tf_idf_log_reg.fit(tf_idf_X_train, Y_train)\n",
    "\n",
    "# making predictions on TF-IDF test set\n",
    "tf_idf_Y_pred = tf_idf_log_reg.predict(tf_idf_X_test)\n",
    "\n",
    "\n",
    "# calculating and printing precision, recall and f1-scores for logistic regression on tf-idf test set\n",
    "tf_idf_precision = precision_score(Y_test, tf_idf_Y_pred)\n",
    "tf_idf_recall = recall_score(Y_test, tf_idf_Y_pred)\n",
    "tf_idf_f1 = f1_score(Y_test, tf_idf_Y_pred)\n",
    "print('TF-IDF: ', tf_idf_precision, tf_idf_recall, tf_idf_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Using Both Features\n",
    "\n",
    "Performance for naive bayes for bag of words:\n",
    "* Precision: 0.837767812655705  \n",
    "* Recall: 0.788575180564675 \n",
    "* F1-Score: 0.8124275222265173\n",
    "\n",
    "Performance for naive bayes for tf-idf:\n",
    "* Precision: 0.8497259591429995 \n",
    "* Recall: 0.800732463142079  \n",
    "* F1-Score: 0.8245020305550185"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW:  0.788575180564675 0.837767812655705 0.8124275222265173\n",
      "TF-IDF:  0.800732463142079 0.8497259591429995 0.8245020305550185\n"
     ]
    }
   ],
   "source": [
    "# Creating and training a Naive-bayes model on Bag-of-words training set\n",
    "bow_nb = MultinomialNB(alpha=5, force_alpha=True)\n",
    "bow_nb.fit(bow_X_train, Y_train)\n",
    "\n",
    "# making predictions on bag-of-words test set\n",
    "bow_Y_pred = bow_nb.predict(bow_X_test)\n",
    "\n",
    "# calculating and printing precision, recall and f1-scores for naive bayes on bag-of-words test set\n",
    "bow_precision = precision_score(Y_test, bow_Y_pred)\n",
    "bow_recall = recall_score(Y_test, bow_Y_pred)\n",
    "bow_f1 = f1_score(Y_test, bow_Y_pred)\n",
    "print('BOW: ', bow_precision, bow_recall, bow_f1)\n",
    "\n",
    "\n",
    "# creating and training a Naive-bayes model on TF-IDF training set\n",
    "tf_idf_nb = MultinomialNB(alpha=1)\n",
    "tf_idf_nb.fit(tf_idf_X_train, Y_train)\n",
    "\n",
    "# making predictions on tf-idf test set\n",
    "tf_idf_Y_pred = tf_idf_nb.predict(tf_idf_X_test)\n",
    "\n",
    "# calculating and printing precision, recall and f1-scores for naive bayes on tf-idf test set\n",
    "tf_idf_precision = precision_score(Y_test, tf_idf_Y_pred)\n",
    "tf_idf_recall = recall_score(Y_test, tf_idf_Y_pred)\n",
    "tf_idf_f1 = f1_score(Y_test, tf_idf_Y_pred)\n",
    "print('TF-IDF: ', tf_idf_precision, tf_idf_recall, tf_idf_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
